module c3tools::ast::lexer;
import c3tools::ast::utils_port;
import std::io;
import std::math;
import std::collections::list;

// Copyright (c) 2019 Christoffer Lerno. All rights reserved.
// Copyright (c) 2025 Alex Veden <i@alexveden.com>. All rights reserved.
// Use of this source code is governed by the MIT license
// a copy of which can be found in the LICENSE_STDLIB file.

alias Lexer = LexerImp;

struct LexerImp
{
    char* file_begin;
    uint file_len;
    char* lexing_start;
    char* current;
    uint current_row;
    uint start_row;
    char* line_start;
    char* start_row_start;
    LexMode mode;
    Token token;
    bool is_whitespace_mode;
}

struct Token (Printable)
{
    TokenType type;
    String value;
    uint row;
    uint col;
    uint offset;
}

fn String Token.to_string(&self, Allocator allocator) @dynamic
{
    @pool() {
        DString builder = dstring::temp();
        builder.appendf("{\n");
        builder.appendf("    type: %s\n", self.type);
        builder.appendf("    value: `%s`\n", self.value);
        builder.appendf("    row: %s\n", self.row);
        builder.appendf("    col: %s\n", self.col);
        builder.appendf("    offset: %s\n", self.offset);
        builder.appendf("}\n");
        return builder.copy_str(allocator);
    };
}

fn void Token.print(&self, bool new_line = false, int padding = 0)
{
    assert(padding >= 0);

    usz plen = 0;
    switch (self.type) {
        case IDENT:
        case CT_IDENT:
        case CT_CONST_IDENT:
        case CT_TYPE_IDENT:
        case HASH_IDENT:
        case HASH_CONST_IDENT:
        case HASH_TYPE_IDENT:
        case CONST_IDENT:
        case TYPE_IDENT:
        case AT_IDENT:
        case AT_TYPE_IDENT:
        case AT_CONST_IDENT:
            plen = io::printf("%s[%s]", lexer::token_type_to_string(self.type), self.value)!!;
            break;
        case STRING:
        case RAW_STRING:
        case INTEGER:
        case REAL:
        case CHAR_LITERAL:
        case BYTES:
            plen = io::printf("%s[%s]", lexer::token_type_to_string(self.type), self.value)!!;
            break;
        case DOCS_START:
        case COMMENT_SINGLE:
        case COMMENT_SINGLE_INLINE:
        case COMMENT_MULTI:
        case COMMENT_MULTI_INLINE:
            plen = io::printf("%s`#%d`", lexer::token_type_to_string(self.type), self.value.len)!!;
            break;
        case EOS:
        case EOF:
        case EMPTY_LINE:
            plen = io::printf("%s", lexer::token_type_to_string(self.type))!!;
            break;
        default:
            plen = io::printf("%s", lexer::token_type_to_string(self.type))!!;
    }

    if (padding > 0) {
        assert(plen > 0);
        for (int i = 0; i < padding - plen; i++) {
            io::print(" ");
        }
    }

    if (new_line) {
        io::print("\n");
    } else {
        io::print(" ");
    }

}

<*
 Initializes new lexer instance

 @param contents: `c3 source contents`
*>
fn void init(Lexer* lexer, String contents)
{
    *lexer = {};
    // Set the current file.
    // lexer.file_begin = lexer.file.contents;
    lexer.file_begin = contents;
    lexer.file_len = contents.len;
    // Set current to beginning.
    lexer.current = lexer.file_begin;
    // Line start is current.
    lexer.line_start = lexer.current;
    // Row number starts at 1
    lexer.current_row = 1;
    // File id is the current file.
    // lexer.tok_span.file_id = lexer.file.file_id;
    // Mode is NORMAL
    lexer.mode = LEX_NORMAL;
    // Set up lexing for a new token.
    begin_new_token(lexer);
}

fn void Lexer.set_whitespace_mode(&self, bool is_enabled)
{
    self.is_whitespace_mode = is_enabled;
}

<*
 Parses next token in a code

 @return `true if token found, false - EOF reached or error`
*>
fn bool Lexer.next_token(&self)
{
    if (self.token.type == EOF) {
        return false;
    }
    // Scan for a token.
    if (lexer_scan_token_inner(self)) return true;
    // Failed, so check if we're at end:
    if (reached_end(self)) return true;
    // Scan through the rest of the text for other invalid tokens:
    bool token_is_ok = false;
    do {
        if (!token_is_ok) {
            // Scan to the end of the line if we have an error.
            while (!reached_end(self) && peek(self) != '\n') next(self);
        }
        token_is_ok = lexer_scan_token_inner(self);
    } while(!reached_end(self));
    // Done.
    return false;
}

<*
 Parses lexer contents and returns new list of all available tokens

 @param allocator: "optional allocator for resulting list"
 @return "list of all tokens in lexer string"
*>
fn List{lexer::Token} Lexer.new_parse_tokens(&self, Allocator allocator = mem)
{
    assert(self.file_begin, "not initialied");
    assert(self.file_begin == self.current, "already processed");

    List{Token} result;
    result.init(allocator);

    while (self.next_token()) {
        result.push(self.token);
    }
    return result;
}

struct TokenData @private
{
    char* lex_start;
    usz lex_len;
    union
    {
        struct
        {
            String string;
        }
        struct
        {
            float value;
        }
        bitstruct : ulong
        {
            bool is_base64 : 0..0;
            ulong bytes_len : 1..63;
        }
        struct
        {
            int128 char_value;
            char width;
        }
    }
}

// --- Lexing general methods.

// Peek at the current character in the buffer.
macro char peek(lexer_) @local
{
    return (*(lexer_).current);
}

// Look at the prev character in the buffer.
macro char prev(lexer_) @local
{
    return ((lexer_).current[-1]);
}

// // Peek one character ahead.
macro char peek_next(lexer_) @local
{
    return ((lexer_).current[1]);
}

//
// // Is the current character '\0' if so we assume we reached the end.
macro bool reached_end(lexer_)
{
    return (lexer_.current >= (lexer_.file_begin + lexer_.file_len) || lexer_.current[0] == '\0');
}

// Step one character forward and return that character
fn char next(Lexer* lexer) @inline
{
    if (@unlikely(*lexer.current == '\n')) {
        lexer.line_start = lexer.current + 1;
        lexer.current_row++;
    }
    lexer.current++;
    if (@unlikely(lexer.current >= (lexer.file_begin + lexer.file_len))) {
        return '\0';
    }
    return (lexer.current)[0];
}

// Backtrack the buffer read one step.
fn void backtrack(Lexer* lexer) @inline @local
{
    lexer.current--;
    if (lexer.current[0] == '\n') {
        lexer.current_row--;
    }
}

// Skip the x next characters.
fn void skip(Lexer* lexer, int steps) @inline @local
{
    assert(steps > 0);
    for (int i = 0; i < steps; i++) {
        next(lexer);
    }
}

// Match a single character â€“ if successful, more one step forward.
fn bool match(Lexer* lexer, char expected) @inline @local
{
    if (lexer.current[0] != expected) return false;
    next(lexer);
    return true;
}

// --- Token creation

fn void begin_new_token(Lexer* lexer) @inline @local
{
    lexer.lexing_start = lexer.current;
    lexer.start_row = lexer.current_row;
    lexer.start_row_start = lexer.line_start;
}

// Add a new regular token.
fn bool new_token(Lexer* lexer, TokenType type, String string) @inline @local
{
    set_generic_token(lexer, type, string);
    return true;
}

/**
 * Allocate data for a token, including source location.
 * This call is doing the basic allocation, with other functions
 * filling out additional information.
 **/
fn void set_generic_token(Lexer* lexer, TokenType type, String value) @inline @local
{
    assert(lexer.lexing_start >= lexer.file_begin);

    lexer.token.type = type;
    // Set the location.
    lexer.token.value = value;
    lexer.token.offset = (uint)(lexer.lexing_start - lexer.file_begin);
    uint line = lexer.start_row;
    uint col;
    uint length;
    if (line == lexer.current_row) {
        // Col is simple difference.
        assert(lexer.lexing_start >= lexer.line_start);
        col = (uint)(lexer.lexing_start - lexer.line_start) + 1;

        // Length is diff between current and start.
        assert(lexer.current >= lexer.lexing_start);
        length = (uint)(lexer.current - lexer.lexing_start);
    } else {
        // For multiline, we grab the diff from the starting line.
        assert(lexer.lexing_start >= lexer.start_row_start);
        col = (uint)(lexer.lexing_start - lexer.start_row_start) + 1;
        // But always set a single token length.
        length = 1;
    }
    lexer.token.col = col;
    lexer.token.row = line;
}

// Error? We simply generate an invalid token and print out the error.
macro bool add_error_token(Lexer* lexer, String message) @local
{

    set_generic_token(lexer, INVALID_TOKEN, message);
    return false;
}

// Error at the start of the lexing, with a single length.
macro bool add_error_token_at_start(Lexer* lexer, String message) @local
{
    set_generic_token(lexer, INVALID_TOKEN, message);
    return false;
}

// Create an error token at a particular place in the file.
// used for pointing out errors in strings etc.
macro bool add_error_token_at(Lexer* lexer, char* loc, isz len, String message) @local
{
    set_generic_token(lexer, INVALID_TOKEN, message);
    return false;
}

// Print an error at the current location.
macro bool add_error_token_at_current(Lexer* lexer, String message) @local
{
    set_generic_token(lexer, INVALID_TOKEN, message);
    return false;
}

// --- Comment parsing

/**
 * Parsing of the "//" line comment - skipping past the end.
 */
fn bool parse_line_comment(Lexer* lexer) @inline @local
{
    backtrack(lexer);
    backtrack(lexer);
    bool has_new_line = (lexer.current == lexer.file_begin);
    begin_new_token(lexer);

    while (!reached_end(lexer) && peek(lexer) != '\n') {
        next(lexer);
    }

    char* cur = lexer.lexing_start - 1;
    while LOOP: (cur >= lexer.file_begin) {
        switch (*cur) {
            case ' ':
            case '\t':
                break;
            case '\n':
                has_new_line = true;
                break LOOP;
            default:
                break LOOP;
        }
        cur--;
    }
    uint len = (uint)(lexer.current - lexer.lexing_start);
    if (lexer.lexing_start[len - 1] == '\n') len--;

    return new_token(
        lexer,
        has_new_line ? COMMENT_SINGLE : COMMENT_SINGLE_INLINE,
        (String)lexer.lexing_start[..len - 1]
    );
}

/**
 * Parse the common / *  * / style multiline comments, allowing nesting.
 **/
fn bool parse_multiline_comment(Lexer* lexer) @inline @local
{
    backtrack(lexer);
    backtrack(lexer);
    bool has_new_line = (lexer.current == lexer.file_begin);
    begin_new_token(lexer);

    int nesting = 0;
    while LOOP: (!reached_end(lexer)) {
        switch (peek(lexer)) {
            case '*':
                if (peek_next(lexer) == '/') {
                    skip(lexer, 2);
                    nesting--;
                    if (nesting == 0) break LOOP;
                    continue;
                }
            case '/':
                if (peek_next(lexer) == '*') {
                    skip(lexer, 2);
                    nesting++;
                    continue;
                }
            case '\0':
                // Reached eof - end.
                return false;
            default:
                break;
        }
        next(lexer);
    }

    char* cur = lexer.lexing_start - 1;
    while LOOP: (cur >= lexer.file_begin) {
        switch (*cur) {
            case ' ':
            case '\t':
                break;
            case '\n':
                has_new_line = true;
            default:
                break LOOP;
        }
        cur--;
    }
    uint len = (uint)(lexer.current - lexer.lexing_start);
    if (lexer.lexing_start[len - 1] == '\n') len--;

    return new_token(
        lexer,
        has_new_line ? COMMENT_MULTI : COMMENT_MULTI_INLINE,
        (String)lexer.lexing_start[..len - 1]
    );
}

/**
 * Skip regular whitespace.
 */
fn bool skip_whitespace(Lexer* lexer) @local
{
    bool has_empty_line = false;
    while LOOP: (!reached_end(lexer)) {
        char c = peek(lexer);
        switch (c) {
            case '/':
                has_empty_line = false;
                if (lexer.mode == LEX_CONTRACTS) return false;
                // The '//' case
                if (peek_next(lexer) == '/') {
                    skip(lexer, 2);
                    if (parse_line_comment(lexer)) {
                        return true;
                    }
                    continue;
                }
                // '/*'
                if (peek_next(lexer) == '*') {
                    skip(lexer, 2);
                    if (parse_multiline_comment(lexer)) {
                        return true;
                    }
                    continue;
                }
                return false;
            case '\n':
                // Contract lexing sees '\n' as a token.
                if (lexer.mode == LEX_CONTRACTS) return false;
                if (has_empty_line) {
                    begin_new_token(lexer);
                    return new_token(lexer, TokenType.EMPTY_LINE, "\n");
                }
                has_empty_line = true;

                nextcase;
            case ' ':
            case '\t':
            case '\f':
                if (lexer.is_whitespace_mode) {
                    begin_new_token(lexer);
                    next(lexer);
                    uint len = (uint)(lexer.current - lexer.lexing_start);
                    return new_token(lexer, SPACE, (String)lexer.lexing_start[..len - 1]);
                }
                nextcase;
            case '\r':  // This is forbidden by c3 grammar, and should be skipped silently
                next(lexer);
                break;
            default:
                has_empty_line = false;
                return false;
        }
    }
    return false;
}

// tries to extent current AT_IDENT
// for handling @if(env::SOO || SOME) @export("something")
fn void Lexer.extend_current_attribute(Lexer* lexer)
{
    Token* result = &lexer.token;
    assert(lexer.token.type == AT_IDENT || lexer.token.type == AT_TYPE_IDENT);
    assert(
        math::abs(lexer.current - lexer.token.value.ptr) < 64,
        "this must be called AT_IDENT parsed"
    );
    assert(lexer.token.value);
    assert(lexer.token.value.ptr >= lexer.file_begin);
    assert(lexer.token.value.ptr < lexer.file_begin + lexer.file_len);

    int nesting = 0;
    int extra_len = 0;
    bool had_nesting = false;
    while LOOP: (!reached_end(lexer)) {
        char c = peek(lexer);
        // io::printf("current char: `%c`\n", c);
        switch (c) {
            case '(':
                nesting++;
                had_nesting = true;
            case ')':
                nesting--;
                if (nesting == 0) next(lexer);
                nextcase default;
            case ' ':
                break;
            default:
                if (nesting <= 0) {
                    if (had_nesting) {
                        result.value = (String)(
                            (char*)result.value.ptr
                        )[..result.value.len + extra_len];
                    }
                    break LOOP;
                }
        }
        extra_len++;
        next(lexer);
    }
    // unreachable();
}

// Parses identifiers. Note that this is a bit complicated here since
// we split identifiers into 2 types + find keywords.
fn bool scan_ident(
    Lexer* lexer, TokenType normal, TokenType const_token, TokenType type_token, char prefix
) @inline @local
{
    TokenType type = INVALID_TOKEN;
    char c;
    while ((c = peek(lexer)) == '_') {
        next(lexer);
    }
    while LOOP: (1) {
        c = peek(lexer);
        switch (c) {
            case 'a'..'z':
                if (type == INVALID_TOKEN) {
                    type = normal;
                } else if (type == const_token) {
                    type = type_token;
                }
                break;
            case 'A'..'Z':
                if (type == INVALID_TOKEN) type = const_token;
                break;
            case '0'..'9':
                if (type == INVALID_TOKEN) return add_error_token(lexer, "A letter must precede any digit");
            case '_':
                break;
            default:
                break LOOP;
        }
        next(lexer);
    }

    uint len = (uint)(lexer.current - lexer.lexing_start);
    if (type == INVALID_TOKEN) {
        if (!prefix && len == 1) return new_token(lexer, UNDERSCORE, "_");
        if (prefix && len == 1) {
            return add_error_token(lexer, "An identifier was expected ... ");
        }
        return add_error_token(lexer, "An identifier may not consist of only '_' characters.");
    }
    switch (type) {
        case RETURN:
            if (lexer.mode == LEX_CONTRACTS) type = IDENT;
            break;
        default:
            break;
    }
    String identifier = (String)lexer.lexing_start[..len - 1];
    TokenType ttype = lexer::token_from_identifier(identifier);
    if (ttype != INVALID_TOKEN) {
        type = ttype;
    }
    return new_token(lexer, type, identifier);
}

// --- Number scanning

/**
 * For C3 we use the practice of f<bit-width> u<bit-width> and i<bit-width>
 * @param lexer
 * @param is_float
 * @return
 */
fn bool scan_number_suffix(Lexer* lexer, bool* is_float) @local
{
    if (prev(lexer) == '_') {
        backtrack(lexer);
        return add_error_token_at_current(
            lexer, "The number ended with '_', which isn't allowed, please remove it."
        );
    }
    char c = peek(lexer);
    if (!utils_port::char_is_alphanum_(c)) return true;
    switch (c | 32) {
        case 'l':
            c = next(lexer);
            if (*is_float) {
                return add_error_token_at_current(
                    lexer, "Integer suffix is not valid for a floating point literal."
                );
            }
            break;
        case 'u':
            if (*is_float) {
                return add_error_token_at_current(
                    lexer, "Integer suffix is not valid for a floating point literal."
                );
            }
            c = next(lexer);
            if ((c | 32) == 'l') {
                c = next(lexer);
                break;
            }
            while (utils_port::char_is_digit(c = peek(lexer))) next(lexer);
            break;
        case 'i':
            if (*is_float) {
                return add_error_token_at_current(
                    lexer, "Integer suffix '%c' is not valid for a floating point literal."
                );
            }
            next(lexer);
            while (utils_port::char_is_digit(c = peek(lexer))) next(lexer);
            break;
        case 'f':
            next(lexer);
            *is_float = true;
            while (utils_port::char_is_digit(c = peek(lexer))) next(lexer);
            break;
        default:
            break;
    }
    if (utils_port::char_is_alphanum_(c)) {
        next(lexer);
        return add_error_token(lexer, "This doesn't seem to be a valid literal.");
    }
    return true;
}

macro bool next_and_check_no_multiple_(Lexer* lexer) @local
{
    if (next(lexer) == '_' && prev(lexer) == '_') {
        return add_error_token_at_current(lexer, "Multiple consecutive '_' are not allowed.");
    }
    return true;
}

/**
 * Parsing octals. Here we depart from the (error prone) C style octals with initial zero e.g. 0231
 * Instead we only support 0o prefix like 0o231. Note that lexing here doesn't actually parse the
 * number itself.
 */
fn bool scan_oct(Lexer* lexer) @local
{
    if (!utils_port::char_is_oct(peek(lexer))) {
        return add_error_token_at_current(
            lexer, "An expression starting with '0o' should be followed by octal numbers (0-7)."
        );
    }
    next(lexer);
    while (utils_port::char_is_oct_or_(peek(lexer))) next_and_check_no_multiple_(lexer);

    if (utils_port::char_is_digit(peek(lexer))) {
        return add_error_token_at_current(
            lexer, "An expression starting with '0o' should be followed by octal numbers (0-7)."
        );
    }
    bool is_float = false;
    if (!scan_number_suffix(lexer, &is_float)) return false;
    if (is_float) {
        return add_error_token(lexer, "Octal literals cannot have a floating point suffix.");
    }
    uint len = (uint)(lexer.current - lexer.lexing_start);
    return new_token(lexer, INTEGER, (String)lexer.lexing_start[..len - 1]);
}

/**
 * Binary style literals e.g. 0b10101011
 **/
fn bool scan_binary(Lexer* lexer) @local
{
    if (!utils_port::char_is_binary(peek(lexer))) {
        return add_error_token_at_current(
            lexer, "An expression starting with '0b' should be followed by binary digits (0-1)."
        );
    }
    next(lexer);
    while (utils_port::char_is_binary_or_(peek(lexer))) next_and_check_no_multiple_(lexer);
    if (utils_port::char_is_digit(peek((lexer)))) {
        return add_error_token_at_current(
            lexer, "An expression starting with '0b' should be followed by binary digits (0-1)."
        );
    }
    bool is_float = false;
    if (!scan_number_suffix(lexer, &is_float)) return false;
    if (is_float) {
        return add_error_token(lexer, "Binary literals cannot have a floating point suffix.");
    }
    // return new_token(lexer, INTEGER, ((ZString)lexer.lexing_start).str_view());
    uint len = (uint)(lexer.current - lexer.lexing_start);
    return new_token(lexer, INTEGER, (String)lexer.lexing_start[..len - 1]);
}

/**
 * Scan the digit after the exponent, e.g +12 or -12 or 12
 * @param lexer
 * @return false if lexing failed.
 */
fn bool scan_exponent(Lexer* lexer) @inline @local
{
    // Step past e/E or p/P
    next(lexer);
    char c = peek(lexer);
    next(lexer);
    // Step past +/-
    if (c == '+' || c == '-') {
        c = peek(lexer);
        next(lexer);
    }
    // Now we need at least one digit
    if (!utils_port::char_is_digit(c)) {
        if (c == 0) {
            backtrack(lexer);
            return add_error_token_at_current(
                lexer, "End of file was reached while parsing the exponent."
            );
        }
        if (c == '\n') return add_error_token(lexer, "End of line was reached while parsing the exponent.");
        if (c < 31 || c > 127) add_error_token(lexer, "An unexpected character was found while parsing the exponent.");
        return add_error_token(
            lexer, "Parsing the floating point exponent failed, because some char is not a number."
        );
    }
    // Step through all the digits.
    while (utils_port::char_is_digit(peek(lexer))) next(lexer);
    return true;
}

/**
 * Scan a hex number, including floating point hex numbers of the format 0x31a31ff.21p12. Note that the
 * exponent is written in decimal.
 **/
fn bool scan_hex(Lexer* lexer) @inline @local
{
    if (!utils_port::char_is_hex(peek(lexer))) {
        return add_error_token_at_current(
            lexer,
            "'0x' starts a hexadecimal number, so the next character should be 0-9, a-f or A-F."
        );
    }
    next(lexer);
    while (utils_port::char_is_hex_or_(peek(lexer))) next_and_check_no_multiple_(lexer);
    bool is_float = false;
    if (peek(lexer) == '.' && peek_next(lexer) != '.') {
        is_float = true;
        next(lexer);
        char c = peek(lexer);
        if (c == '_') return add_error_token_at_current(lexer, "'_' is not allowed directly after decimal point, try removing it.");
        if (utils_port::char_is_hex(c)) next(lexer);
        while (utils_port::char_is_hex_or_(peek(lexer))) next_and_check_no_multiple_(lexer);
    }
    char c = peek(lexer);
    if (c == 'p' || c == 'P') {
        is_float = true;
        if (!scan_exponent(lexer)) return false;
    }
    if (!scan_number_suffix(lexer, &is_float)) return false;
    // return new_token(lexer, is_float ? REAL : INTEGER, ((ZString)lexer.lexing_start).str_view());
    uint len = (uint)(lexer.current - lexer.lexing_start);
    return new_token(lexer, is_float ? REAL : INTEGER, (String)lexer.lexing_start[..len - 1]);

}

/**
 * Scans integer and float decimal values.
 */
fn bool scan_dec(Lexer* lexer) @inline @local
{
    assert(utils_port::char_is_digit(peek(lexer)));

    // Walk through the digits, we don't need to worry about
    // initial _ because we only call this if we have a digit initially.
    while (utils_port::char_is_digit_or_(peek(lexer))) next_and_check_no_multiple_(lexer);

    // Assume no float.
    bool is_float = false;

    // If we have a single dot, we assume that we have a float.
    // Note that this current parsing means we can't have functions on
    // literals, like "123.sizeof", but we're fine with that.
    if (peek(lexer) == '.' && peek_next(lexer) != '.') {
        is_float = true;
        // Step past '.'
        next(lexer);
        // Check our rule to disallow 123._32
        char c = peek(lexer);
        if (c == '_') return add_error_token_at_current(lexer, "'_' is not allowed directly after decimal point, try removing it.");
        // Now walk until we see no more digits.
        // This allows 123. as a floating point number.
        while (utils_port::char_is_digit_or_(peek(lexer))) next_and_check_no_multiple_(lexer);
    }
    char c = peek(lexer);
    // We might have an exponential. We allow 123e1 and 123.e1 as floating point, so
    // just set it to floating point and check the exponential.
    if (c == 'e' || c == 'E') {
        is_float = true;
        if (!scan_exponent(lexer)) return false;
    }
    if (!scan_number_suffix(lexer, &is_float)) return false;
    // return new_token(lexer, is_float ? REAL : INTEGER, ((ZString)lexer.lexing_start).str_view());
    uint len = (uint)(lexer.current - lexer.lexing_start);
    return new_token(lexer, is_float ? REAL : INTEGER, (String)lexer.lexing_start[..len - 1]);
}

/**
 * Scan a digit, switching on initial zero on possible parsing schemes:
 * 0x... . Hex
 * 0o... . Octal
 * 0b... . Binary
 *
 * Default is decimal.
 *
 * It's actually pretty simple to add encoding schemes here, so for example Base64 could
 * be added.
 */
fn bool scan_digit(Lexer* lexer) @inline @local
{
    if (peek(lexer) == '0') {
        switch (peek_next(lexer)) {
            case 'x':
            case 'X':
                skip(lexer, 2);
                return scan_hex(lexer);
            case 'o':
            case 'O':
                skip(lexer, 2);
                return scan_oct(lexer);
            case 'b':
            case 'B':
                skip(lexer, 2);
                return scan_binary(lexer);
            default:
                break;
        }
    }
    return scan_dec(lexer);
}

// --- Character & string scan

fn bool scan_char(Lexer* lexer) @inline @local
{
    // Handle the problem with zero size character literal first.
    if (match(lexer, '\'')) {
        return add_error_token(lexer, "The character literal was empty.");
    }

    int width = 1;
    char c;

    while LOOP: (!match(lexer, '\'')) {
        c = peek(lexer);
        next(lexer);
        // End of file may occur:
        switch (c) {
            case '\0':
                return add_error_token_at_start(lexer, "The character literal did not terminate.");
            case '\\':  // escape sequence
                c = peek(lexer);
                if (c == '\'' || c == '\\') {
                    next(lexer);
                    width++;
                }
        }
        width++;
    }
    return new_token(lexer, CHAR_LITERAL, (String)lexer.lexing_start[..width]);
}

fn void consume_to_end_quote(Lexer* lexer) @inline @local
{
    char c;
    while ((c = peek(lexer)) != '\0' && c != '"') {
        next(lexer);
    }
}

fn bool scan_string(Lexer* lexer) @inline @local
{
    char c = 0;
    char* current = lexer.current;
    while ((c = *(current++)) != '"') {
        if (c == '\n' || c == '\0') {
            current++;
            break;
        }
        if (c == '\\') {
            c = *current;
            if (c != '\n' && c != '\0') current++;
            continue;
        }
    }
    char* end = current - 1;
    usz len = 0;

    // NOTE: inlcuding previous "
    backtrack(lexer);
    begin_new_token(lexer);
    while (lexer.current < end) {
        c = peek(lexer);
        next(lexer);
        if (c == '\0' || (c == '\\' && peek(lexer) == '\0')) {
            if (c == '\0') backtrack(lexer);
            add_error_token_at_start(
                lexer,
                "The end of the file was reached ""while parsing the string. ""Did you forget (or accidentally add) a '\"' somewhere?"
            );
            consume_to_end_quote(lexer);
            return false;
        }
        if (c == '\n' || (c == '\\' && peek(lexer) == '\n')) {

            backtrack(lexer);
            add_error_token_at_start(
                lexer,
                "The end of the line was reached ""while parsing the string. ""Did you forget (or accidentally add) a '\"' somewhere?"
            );
            consume_to_end_quote(lexer);
            return false;
        }
        len++;
    }
    // Skip the `"`
    next(lexer);
    return new_token(lexer, STRING, len > 0 ? (String)lexer.lexing_start[..len] : "");

}

fn bool scan_raw_string(Lexer* lexer) @inline @local
{
    char c;
    while (1) {
        c = peek(lexer);
        next(lexer);
        if (c == '`' && peek(lexer) != '`') break;
        if (c == '\0') {
            return add_error_token_at_start(
                lexer,
                "Reached the end of the file looking for ""the end of the raw string that starts ""here. Did you forget a '`' somewhere?"
            );
        }
        if (c == '`') next(lexer);
    }
    char* current = lexer.lexing_start;
    char* end = lexer.current;
    usz len = (usz)(end - current);
    new_token(lexer, RAW_STRING, (String)current[0..len - 1]);
    return true;
}

fn bool scan_hex_array(Lexer* lexer) @inline @local
{
    char start_char = peek(lexer);
    next(lexer);  // Step past ' or " `
    char c;
    ulong len = 2;
    while (1) {
        c = peek(lexer);
        if (c == 0) {
            return add_error_token_at_current(
                lexer, "The hex string seems to be missing a terminating "
            );
        }

        if (c == start_char) break;

        if (utils_port::char_is_hex(c)) {
            next(lexer);
            len++;
            continue;
        }
        if (utils_port::char_is_whitespace(c)) {
            next(lexer);
            continue;
        }
        if (c > ' ' && c < 127) {
            return add_error_token_at_current(
                lexer,
                " isn't a valid hexadecimal digit, all digits should be a-z, A-Z and 0-9.",
            );
        }
        return add_error_token_at_current(
            lexer, "This isn't a valid hexadecimal digit, all digits should be a-z, A-Z and 0-9."
        );
    }
    next(lexer);
    if (len % 2) {
        return add_error_token(
            lexer, "The hexadecimal string is not an even length, did you miss a digit somewhere?"
        );
    }

    char* current = lexer.lexing_start;
    char* end = lexer.current;
    len = (usz)(end - current - 1);

    new_token(lexer, BYTES, (String)lexer.lexing_start[..len]);
    return true;
}

// Scan b64"abc=" and b64'abc='
fn bool scan_base64(Lexer* lexer) @inline @local
{
    next(lexer);  // Step past 6
    next(lexer);  // Step past 4
    char start_char = peek(lexer);
    next(lexer);  // Step past ' or " or `
    char c;
    uint end_len = 0;
    ulong len = 4;
    while (1) {
        c = peek(lexer);
        if (c == 0) {
            return add_error_token_at_start(
                lexer, "The base64 string seems to be missing a terminating"
            );
        }
        next(lexer);
        if (c == start_char) break;
        if (utils_port::char_is_base64(c)) {
            if (end_len) {
                return add_error_token_at_current(lexer, "can't be placed after an ending '='");
            }
            len++;
            continue;
        }
        if (c == '=') {
            if (end_len > 1) {
                return add_error_token_at_current(
                    lexer, "There cannot be more than 2 '=' at the end of a base64 string."
                );
            }
            end_len++;
            continue;
        }
        if (!utils_port::char_is_whitespace(c)) {
            if (c < ' ' || c > 127) {
                return add_error_token_at_current(
                    lexer, "A valid base64 character was expected here."
                );
            }
            return add_error_token_at_current(lexer, "not a valid base64 character.");
        }
    }
    char* current = lexer.lexing_start;
    char* end = lexer.current;
    len = (usz)(end - current - 1);
    new_token(lexer, BYTES, (String)lexer.lexing_start[..len]);
    return true;
}

// --- Lexer doc lexing

/**
 * Parse the <* *> directives comments
 **/
fn bool parse_doc_start(Lexer* lexer) @local
{
    bool may_have_contract = true;
    bool has_new_line = false;
    // Let's loop until we find the end or the contract.
    while LOOP: (!reached_end(lexer)) {
        char c = peek(lexer);
        switch (c) {
            case '\n':
                may_have_contract = true;
                has_new_line = true;
                next(lexer);
                continue;
            case ' ':
            case '\t':
                next(lexer);
                continue;
            case '*':
                // We might have <* Hello *>
                // if (peek_next(lexer) == '>') goto EXIT;
                if (peek_next(lexer) == '>') break LOOP;
                may_have_contract = false;
                next(lexer);
                continue;
            case '@':
                if (may_have_contract && utils_port::char_is_lower(peek_next(lexer))) {
                    // Found a contract
                    // goto EXIT;
                    break LOOP;
                }
                nextcase;
            default:
                may_have_contract = false;
                next(lexer);
                continue;
        }
    }
    // EXIT:;
    // Now we either found:
    // 1. "<* foo \n @param"
    // 2. "<* foo *>"
    // 3. "<* foo <eof>"
    //
    // In any case we can consider this having reached "the contracts"
    if (has_new_line) {
        while LOOP: (lexer.current - 1 > lexer.lexing_start) {
            switch (*(lexer.current - 1)) {
                case ' ':
                case '\t':
                    lexer.current--;
                case '\n':
                default:
                    break LOOP;
            }

        }
    }
    lexer.mode = LEX_CONTRACTS;
    uint len = (uint)(lexer.current - lexer.lexing_start);
    return new_token(lexer, DOCS_START, (String)lexer.lexing_start[..len - 1]);
}

fn bool lexer_scan_token_inner(Lexer* lexer) @local
{
    if (skip_whitespace(lexer)) {
        // Now skip the whitespace (but check for comments).
        return true;
    }

    // Point start to the first non-whitespace character.
    begin_new_token(lexer);

    if (reached_end(lexer)) {
        new_token(lexer, EOF, "\n");
        return false;
    }

    ichar c = peek(lexer);
    next(lexer);
    switch (c) {
        case '\n':
            assert(lexer.mode == LEX_CONTRACTS);
            return new_token(lexer, DOCS_EOL, "\n");
        case '@':
            if (utils_port::char_is_letter_(peek(lexer))) {
                return scan_ident(lexer, AT_IDENT, AT_CONST_IDENT, AT_TYPE_IDENT, '@');
            }
            return new_token(lexer, AT, "@");
        case '\'':
            return scan_char(lexer);
        case '`':
            return scan_raw_string(lexer);
        case '"':
            return scan_string(lexer);
        case '#':
            return scan_ident(lexer, HASH_IDENT, HASH_CONST_IDENT, HASH_TYPE_IDENT, '#');
        case '$':
            if (match(lexer, '$')) {
                if (utils_port::char_is_letter(peek(lexer))) {
                    return scan_ident(lexer, BUILTIN, BUILTIN, BUILTIN, '#');
                }
                return add_error_token_at_current(lexer, "Expected a letter after $$.");
            } else {
                return scan_ident(lexer, CT_IDENT, CT_CONST_IDENT, CT_TYPE_IDENT, '$');
            }
        case ',':
            return new_token(lexer, COMMA, ",");
        case ';':
            return new_token(lexer, EOS, ";");
        case '{':
            return match(
                lexer, '|'
            ) ? new_token(
                lexer, LBRAPIPE, "{|"
            ) : new_token(
                lexer, LBRACE, "{"
            );
        case '}':
            return new_token(lexer, RBRACE, "}");
        case '(':
            return match(
                lexer, '<'
            ) ? new_token(
                lexer, LGENPAR, "(<"
            ) : new_token(
                lexer, LPAREN, "("
            );
        case ')':
            return new_token(lexer, RPAREN, ")");
        case '[':
            if (match(lexer, '<')) return new_token(lexer, LVEC, "[<");
            return new_token(lexer, LBRACKET, "[");
        case ']':
            return new_token(lexer, RBRACKET, "]");
        case '.':
            if (match(lexer, '.')) {
                if (match(lexer, '.')) return new_token(lexer, ELLIPSIS, "...");
                return new_token(lexer, DOTDOT, "..");
            }
            return new_token(lexer, DOT, ".");
        case '~':
            return new_token(lexer, BIT_NOT, "~");
        case ':':
            return match(lexer, ':') ? new_token(lexer, SCOPE, "::") : new_token(lexer, COLON, ":");
        case '!':
            if (match(lexer, '!')) return new_token(lexer, BANGBANG, "!!");
            return match(
                lexer, '='
            ) ? new_token(
                lexer, NOT_EQUAL, "!="
            ) : new_token(
                lexer, BANG, "!"
            );
        case '/':
            return match(
                lexer, '='
            ) ? new_token(
                lexer, DIV_ASSIGN, "/="
            ) : new_token(
                lexer, DIV, "/"
            );
        case '*':
            if (lexer.mode == LEX_CONTRACTS && match(lexer, '>')) {
                lexer.mode = LEX_NORMAL;
                return new_token(lexer, DOCS_END, "*>");
            }
            return match(
                lexer, '='
            ) ? new_token(
                lexer, MULT_ASSIGN, "*="
            ) : new_token(
                lexer, STAR, "*"
            );
        case '=':
            if (match(lexer, '>')) return new_token(lexer, IMPLIES, "=>");
            return match(lexer, '=') ? new_token(lexer, EQEQ, "==") : new_token(lexer, EQ, "=");
        case '^':
            return match(
                lexer, '='
            ) ? new_token(
                lexer, BIT_XOR_ASSIGN, "^="
            ) : new_token(
                lexer, BIT_XOR, "^"
            );
        case '?':
            if (match(lexer, '?')) return new_token(lexer, QUESTQUEST, "??");
            return match(
                lexer, ':'
            ) ? new_token(
                lexer, ELVIS, "?:"
            ) : new_token(
                lexer, QUESTION, "?"
            );
        case '<':
            if (match(lexer, '<')) {
                if (match(lexer, '=')) return new_token(lexer, SHL_ASSIGN, "<<=");
                return new_token(lexer, SHL, "<<");
            }
            if (lexer.mode == LEX_NORMAL && match(lexer, '*')) {
                return parse_doc_start(lexer);
            }
            return match(lexer, '=') ? new_token(lexer, LESS_EQ, "<=") : new_token(lexer, LESS, "<");
        case '>':
            if (match(lexer, '>')) {
                if (match(lexer, '=')) return new_token(lexer, SHR_ASSIGN, ">>=");
                return new_token(lexer, SHR, ">>");
            }
            if (match(lexer, ')')) return new_token(lexer, RGENPAR, ">)");
            if (match(lexer, ']')) return new_token(lexer, RVEC, ">]");
            return match(
                lexer, '='
            ) ? new_token(
                lexer, GREATER_EQ, ">="
            ) : new_token(
                lexer, GREATER, ">"
            );
        case '%':
            return match(
                lexer, '='
            ) ? new_token(
                lexer, MOD_ASSIGN, "%="
            ) : new_token(
                lexer, MOD, "%"
            );
        case '&':
            if (match(lexer, '&')) {
                return match(
                    lexer, '&'
                ) ? new_token(
                    lexer, CT_AND, "&&&"
                ) : new_token(
                    lexer, AND, "&&"
                );
            }
            return match(
                lexer, '='
            ) ? new_token(
                lexer, BIT_AND_ASSIGN, "&="
            ) : new_token(
                lexer, AMP, "&"
            );
        case '|':
            if (match(lexer, '}')) return new_token(lexer, RBRAPIPE, "|}");
            if (match(lexer, '|')) {
                return match(
                    lexer, '|'
                ) ? new_token(
                    lexer, CT_OR, "|||"
                ) : new_token(
                    lexer, OR, "||"
                );
            }
            return match(
                lexer, '='
            ) ? new_token(
                lexer, BIT_OR_ASSIGN, "|="
            ) : new_token(
                lexer, BIT_OR, "|"
            );
        case '+':
            if (match(lexer, '+')) {
                if (match(lexer, '+')) return new_token(lexer, CT_CONCAT, "+++");
                return new_token(lexer, PLUSPLUS, "++");
            }
            if (match(lexer, '=')) return new_token(lexer, PLUS_ASSIGN, "+=");
            return new_token(lexer, PLUS, "+");
        case '-':
            if (match(lexer, '>')) return new_token(lexer, ARROW, ".");
            if (match(lexer, '-')) return new_token(lexer, MINUSMINUS, "--");
            if (match(lexer, '=')) return new_token(lexer, MINUS_ASSIGN, "-=");
            return new_token(lexer, MINUS, "-");
        case 'x':
            if ((peek(lexer) == '"' || peek(lexer) == '\'' || peek(lexer) == '`')) {
                return scan_hex_array(lexer);
            }
            // goto IDENT;
            nextcase '_';
        case 'b':
            if (
                peek(lexer) == '6' &&
                peek_next(lexer) == '4' &&
                (lexer.current[2] == '\'' || lexer.current[2] == '"' || lexer.current[2] == '`')
            ) {
                return scan_base64(lexer);
            }
            // goto IDENT;
            nextcase '_';
        case '_':
            // IDENT:
            backtrack(lexer);
            return scan_ident(lexer, IDENT, CONST_IDENT, TYPE_IDENT, 0);
        default:
            if (c >= '0' && c <= '9') {
                backtrack(lexer);
                return scan_digit(lexer);
            }
            if (c >= 'a' && c <= 'z') nextcase '_';  // goto IDENT;
            if (c >= 'A' && c <= 'Z') nextcase '_';  // goto IDENT;
            if (c < 0) {
                return add_error_token(
                    lexer,
                    "The 0x ?? character may not be placed outside of a string or comment, did you forget a \" somewhere?"
                );
            }
            return add_error_token(
                lexer,
                "char may not be placed outside of a string or comment, did you perhaps forget a \" somewhere?"
            );

    }
}

const int MAX_SOURCE_LOCATION_LEN = 255;

enum TokenType : uint
{
    INVALID_TOKEN,

    // Single-character tokens.
    AMP,  // &
    AT,  // @
    BANG,  // !
    BIT_NOT,  // ~
    BIT_OR,  // =
    BIT_XOR,  // ^
    COLON,  // :
    COMMA,  // ,
    EOS,  // ;
    EMPTY_LINE,  // a line with only white spaces (useful for code formatting)
    EQ,  // =
    GREATER,  // >
    DIV,  // /
    DOLLAR,  // $
    DOT,  // .
    HASH,  // #
    LESS,  // <
    LBRACE,  // {
    LBRACKET,  // [
    LPAREN,  // (
    MINUS,  // -
    MOD,  // %
    PLUS,  // +
    QUESTION,  // ?
    RBRACE,  // }
    RBRACKET,  // ]
    RPAREN,  // )
    STAR,  // *
    UNDERSCORE,  // _
    SPACE,  // ' '

    // two character tokens.
    AND,  // &&
    ARROW,  // -> // Not used but reserved
    BANGBANG,  // !!
    BIT_AND_ASSIGN,  // &=
    BIT_OR_ASSIGN,  // |=
    BIT_XOR_ASSIGN,  // ^=
    DIV_ASSIGN,  // /=
    DOTDOT,  // ..
    BUILTIN,  // $$
    ELVIS,  // ?:
    EQEQ,  // ==
    GREATER_EQ,  // >=
    IMPLIES,  // =>
    LESS_EQ,  // <=
    LBRAPIPE,  // {|
    LGENPAR,  // (<
    LVEC,  // [<
    MINUS_ASSIGN,  // -=
    MINUSMINUS,  // --
    MOD_ASSIGN,  // %=
    MULT_ASSIGN,  // *=
    NOT_EQUAL,  // !=
    OR,  // ||
    PLUS_ASSIGN,  // +=
    PLUSPLUS,  // ++
    RBRAPIPE,  // |}
    RGENPAR,  // >)
    RVEC,  // >]
    QUESTQUEST,  // ??
    SCOPE,  // ::
    SHL,  // <<
    SHR,  // >>

    // Three or more
    ELLIPSIS,  // ...
    SHL_ASSIGN,  // <<=
    SHR_ASSIGN,  // >>=
    CT_AND,  // &&&
    CT_CONCAT,  // +++
    CT_OR,  // |||
    // Literals.
    IDENT,  // Any normal ident.
    CONST_IDENT,  // Any purely uppercase ident,
    TYPE_IDENT,  // Any ident on the format FooBar or __FooBar

    // We want to parse $foo separately,
    // otherwise we allow things like "$ foo" which would be pretty bad.
    CT_IDENT,  // $foobar
    CT_CONST_IDENT,  // $FOOBAR
    CT_TYPE_IDENT,  // $Foobar

    // We want to parse #foo separately.
    HASH_IDENT,  // #foobar
    HASH_CONST_IDENT,  // #FOOBAR
    HASH_TYPE_IDENT,  // #Foobar

    AT_IDENT,  // @macro
    AT_CONST_IDENT,  // @MACRO
    AT_TYPE_IDENT,  // @Macro

    COMMENT_SINGLE_INLINE,  // //single INLINE comment
    COMMENT_SINGLE,  // //single line comment
    COMMENT_MULTI,  //  /* multiline on new line */
    COMMENT_MULTI_INLINE,  //  some /* multiline on the same line */

    STRING,  // "Teststring"
    RAW_STRING,  // `Teststring`
    INTEGER,  // 123 0x23 0b10010 0o327
    CHAR_LITERAL,  // 'a' 'FO' 'BARS' '\u1232'
    REAL,  // 0x23.2p-2a 43.23e23
    BYTES,  // Base64 or Hex

    DOC_COMMENT,  // Doc Comment start

    // Basic types names
    VOID,
    // FIRST_KEYWORD = TOKEN_VOID,
    BOOL,
    CHAR,
    DOUBLE,
    FLOAT,
    FLOAT16,
    BFLOAT,
    INT128,
    ICHAR,
    INT,
    IPTR,
    ISZ,
    LONG,
    SHORT,
    UINT128,
    UINT,
    ULONG,
    UPTR,
    USHORT,
    USZ,
    FLOAT128,
    ANY,
    ANYFAULT,
    TYPEID,

    // Keywords
    ASSERT,
    ASM,
    STRUCT,
    ALIAS,
    DISTINCT,
    BITSTRUCT,
    FAULT,
    UNION,
    BREAK,
    CASE,
    CATCH,
    CONTINUE,
    DEFAULT,
    DEFER,
    DO,
    ELSE,
    ENUM,
    EXTERN,
    FALSE,
    FOR,
    FOREACH,
    FOREACH_R,
    FN,
    TLOCAL,
    IF,
    INLINE,
    IMPORT,
    MACRO,
    MODULE,
    NEXTCASE,
    NULL,
    INTERFACE,
    RETURN,
    STATIC,
    SWITCH,
    TRUE,
    TRY,
    VAR,
    WHILE,
    CONST,
    // LAST_NON_CT_KEYWORD = TOKEN_WHILE,

    CT_ALIGNOF,  // $alignof
    CT_ANDFN,  // $and
    CT_APPEND,  // $append
    CT_ASSERT,  // $assert
    CT_ASSIGNABLE,  // $assignable
    CT_CASE,  // $case
    CT_CONCATFN,  // $concat
    CT_DEFAULT,  // $default
    CT_DEFINED,  // $defined
    CT_ECHO,  // $echo
    CT_ELSE,  // $else
    CT_EMBED,  // $embed
    CT_ENDFOR,  // $endfor
    CT_ENDFOREACH,  // $endforeach
    CT_ENDIF,  // $endif
    CT_ENDSWITCH,  // $endswitch
    CT_EVAL,  // $eval
    CT_EVALTYPE,  // $evaltype
    CT_ERROR,  // $error
    CT_EXEC,  // $exec
    CT_EXTNAMEOF,  // $extnameof
    CT_FEATURE,  // $feature
    CT_FOR,  // $for
    CT_FOREACH,  // $foreach
    CT_IF,  // $if
    CT_INCLUDE,  // $include
    CT_IS_CONST,  // $is_const
    CT_NAMEOF,  // $nameof
    CT_OFFSETOF,  // $offsetof
    CT_ORFN,  // $or
    CT_QNAMEOF,  // $qnameof
    CT_SIZEOF,  // $sizeof
    CT_STRINGIFY,  // $stringify
    CT_SWITCH,  // $switch
    CT_TYPEFROM,  // $typefrom
    CT_TYPEOF,  // $typeof
    CT_VACOUNT,  // $vacount
    CT_VATYPE,  // $vatype
    CT_VACONST,  // $vaconst,
    CT_VAREF,  // $varef,
    CT_VAARG,  // $vaarg,
    CT_VAEXPR,  // $vaexpr,
    CT_VASPLAT,  // $vasplat,
    // LAST_KEYWORD = TOKEN_CT_VASPLAT,
    DOCS_START,  // <*
    DOCS_END,  // *>
    DOCS_EOL,

    EOF,  // \n - SHOULD ALWAYS BE THE LAST TOKEN.

    // LAST = TOKEN_EOF,
}

union SourceSpan
{
    struct
    {
        char length;
        uint col;
        uint row;
    }
    ulong a;
}

enum LexMode
{
    LEX_NORMAL,
    LEX_CONTRACTS,
}

fn String token_type_to_string(TokenType type)
{
    switch (type) {
        case INVALID_TOKEN:
            return "INVALID_TOKEN";

            // One character tokens
        case AMP:
            return "&";
        case AT:
            return "@";
        case BIT_NOT:
            return "~";
        case BIT_OR:
            return "|";
        case BIT_XOR:
            return "^";
        case COLON:
            return ":";
        case COMMA:
            return ",";
        case DIV:
            return "/";
        case DOLLAR:
            return "$";
        case DOT:
            return ".";
        case EOS:
            return ";";
        case EMPTY_LINE:
            return "<EMPTY_LINE>";
        case EQ:
            return "=";
        case GREATER:
            return ">";
        case HASH:
            return "#";
        case LBRACE:
            return "{";
        case LBRACKET:
            return "[";
        case LESS:
            return "<";
        case LPAREN:
            return "(";
        case MINUS:
            return "-";
        case MOD:
            return "%";
        case BANG:
            return "!";
        case PLUS:
            return "+";
        case QUESTION:
            return "?";
        case RBRACE:
            return "}";
        case RBRACKET:
            return "]";
        case RPAREN:
            return ")";
        case STAR:
            return "*";
        case UNDERSCORE:
            return "_";
        case SPACE:
            return " ";

            // Two character tokens
        case AND:
            return "&&";
        case ARROW:
            return "->";
        case BIT_AND_ASSIGN:
            return "&=";
        case BIT_OR_ASSIGN:
            return "|=";
        case BIT_XOR_ASSIGN:
            return "^=";
        case BUILTIN:
            return "$$";
        case CT_AND:
            return "&&&";
        case CT_OR:
            return "|||";
        case CT_CONCAT:
            return "+++";
        case DIV_ASSIGN:
            return "/=";
        case DOTDOT:
            return "..";
        case ELVIS:
            return "?:";
        case EQEQ:
            return "==";
        case GREATER_EQ:
            return ">=";
        case IMPLIES:
            return "=>";
        case LESS_EQ:
            return "<=";
        case LBRAPIPE:
            return "{|";
        case LGENPAR:
            return "(<";
        case LVEC:
            return "[<";
        case MINUS_ASSIGN:
            return "-=";
        case MINUSMINUS:
            return "--";
        case MULT_ASSIGN:
            return "*=";
        case MOD_ASSIGN:
            return "%=";
        case NOT_EQUAL:
            return "!=";
        case OR:
            return "||";
        case PLUS_ASSIGN:
            return "+=";
        case PLUSPLUS:
            return "++";
        case QUESTQUEST:
            return "??";
        case RBRAPIPE:
            return "|}";
        case RGENPAR:
            return ">)";
        case RVEC:
            return ">]";
        case SCOPE:
            return "::";
        case SHL:
            return "<<";
        case SHR:
            return ">>";
        case BANGBANG:
            return "!!";

            // Three character tokens
        case ELLIPSIS:
            return "...";
        case SHL_ASSIGN:
            return "<<=";
        case SHR_ASSIGN:
            return ">>=";

            // Identifiers
        case IDENT:
            return "IDENT";
        case CT_IDENT:
            return "CT_IDENT";
        case CT_CONST_IDENT:
            return "CT_CONST_IDENT";
        case CT_TYPE_IDENT:
            return "CT_TYPE_IDENT";
        case HASH_IDENT:
            return "HASH_IDENT";
        case HASH_CONST_IDENT:
            return "HASH_CONST_IDENT";
        case HASH_TYPE_IDENT:
            return "HASH_TYPE_IDENT";
        case CONST_IDENT:
            return "CONST_IDENT";
        case TYPE_IDENT:
            return "TYPE_IDENT";

        case AT_IDENT:
            return "MACRO_IDENT";
        case AT_TYPE_IDENT:
            return "MACRO_TYPE_IDENT";
        case AT_CONST_IDENT:
            return "MACRO_CONST_IDENT";

            // Values
        case STRING:
            return "STRING";
        case RAW_STRING:
            return "RAW_STRING";
        case COMMENT_SINGLE:
            return "COMMENT_SINGLE";
        case COMMENT_SINGLE_INLINE:
            return "COMMENT_SINGLE_INLINE";
        case COMMENT_MULTI:
            return "COMMENT_MULTI";
        case COMMENT_MULTI_INLINE:
            return "COMMENT_MULTI_INLINE";
        case INTEGER:
            return "INTEGER";
        case REAL:
            return "FLOAT";
        case CHAR_LITERAL:
            return "CHAR_LITERAL";
        case BYTES:
            return "BYTES";

            // Comments
        case DOC_COMMENT:
            return "DOC_COMMENT";

            // Keywords
        case ANYFAULT:
            return "anyfault";
        case ASM:
            return "asm";
        case ASSERT:
            return "assert";
        case BITSTRUCT:
            return "bitstruct";
        case BREAK:
            return "break";
        case CASE:
            return "case";
        case CATCH:
            return "catch";
        case CONST:
            return "const";
        case CONTINUE:
            return "continue";
        case ALIAS:
            return "alias";
        case DEFAULT:
            return "default";
        case DEFER:
            return "defer";
        case DISTINCT:
            return "distinct";
        case DO:
            return "do";
        case ELSE:
            return "else";
        case ENUM:
            return "enum";
        case EXTERN:
            return "extern";
        case FALSE:
            return "false";
        case FAULT:
            return "fault";
        case FOR:
            return "for";
        case FOREACH:
            return "foreach";
        case FOREACH_R:
            return "foreach_r";
        case FN:
            return "fn";
        case IF:
            return "if";
        case INLINE:
            return "inline";
        case INTERFACE:
            return "interface";
        case IMPORT:
            return "import";
        case MACRO:
            return "macro";
        case MODULE:
            return "module";
        case NEXTCASE:
            return "nextcase";
        case NULL:
            return "null";
        case RETURN:
            return "return";
        case STATIC:
            return "static";
        case STRUCT:
            return "struct";
        case SWITCH:
            return "switch";
        case TLOCAL:
            return "tlocal";
        case TRUE:
            return "true";
        case TRY:
            return "try";
        case TYPEID:
            return "typeid";
        case UNION:
            return "union";
        case VAR:
            return "var";
        case WHILE:
            return "while";

            // Named types
        case VOID:
            return "void";
        case ANY:
            return "any";
        case BOOL:
            return "bool";
        case FLOAT128:
            return "float128";
        case DOUBLE:
            return "double";
        case FLOAT:
            return "float";
        case BFLOAT:
            return "bfloat";
        case FLOAT16:
            return "float16";
        case LONG:
            return "long";
        case ULONG:
            return "ulong";
        case INT128:
            return "int128";
        case UINT128:
            return "uint128";
        case INT:
            return "int";
        case UINT:
            return "uint";
        case SHORT:
            return "short";
        case USHORT:
            return "ushort";
        case ICHAR:
            return "ichar";
        case CHAR:
            return "char";
        case ISZ:
            return "isz";
        case USZ:
            return "usz";
        case IPTR:
            return "iptr";
        case UPTR:
            return "uptr";
        case DOCS_START:
            return "<*";
        case DOCS_END:
            return "*>";
        case CT_ALIGNOF:
            return "$alignof";
        case CT_ANDFN:
            return "$and";
        case CT_APPEND:
            return "$append";
        case CT_ASSERT:
            return "$assert";
        case CT_ASSIGNABLE:
            return "$assignable";
        case CT_CASE:
            return "$case";
        case CT_CONCATFN:
            return "$concat";
        case CT_DEFAULT:
            return "$default";
        case CT_DEFINED:
            return "$defined";
        case CT_ELSE:
            return "$else";
        case CT_EMBED:
            return "$embed";
        case CT_ENDIF:
            return "$endif";
        case CT_ENDSWITCH:
            return "$endswitch";
        case CT_ENDFOR:
            return "$endfor";
        case CT_ENDFOREACH:
            return "$endforeach";
        case CT_EVAL:
            return "$eval";
        case CT_EVALTYPE:
            return "$evaltype";
        case CT_ERROR:
            return "$error";
        case CT_EXEC:
            return "$exec";
        case CT_EXTNAMEOF:
            return "$extnameof";
        case CT_FEATURE:
            return "$feature";
        case CT_FOR:
            return "$for";
        case CT_FOREACH:
            return "$foreach";
        case CT_IF:
            return "$if";
        case CT_IS_CONST:
            return "$is_const";
        case CT_INCLUDE:
            return "$include";
        case CT_VACOUNT:
            return "$vacount";
        case CT_VATYPE:
            return "$vatype";
        case CT_VACONST:
            return "$vaconst";
        case CT_VAARG:
            return "$vaarg";
        case CT_VAREF:
            return "$varef";
        case CT_VAEXPR:
            return "$vaexpr";
        case CT_VASPLAT:
            return "$vasplat";
        case CT_NAMEOF:
            return "$nameof";
        case CT_OFFSETOF:
            return "$offsetof";
        case CT_ORFN:
            return "$or";
        case CT_QNAMEOF:
            return "$qnameof";
        case CT_SIZEOF:
            return "$sizeof";
        case CT_SWITCH:
            return "$switch";
        case CT_TYPEFROM:
            return "$typefrom";
        case CT_TYPEOF:
            return "$typeof";
        case CT_STRINGIFY:
            return "$stringify";
        case CT_ECHO:
            return "$echo";
        case DOCS_EOL:
            return "<EOL>";
        case EOF:
            return "EOF";
        default:
            unreachable();

    }
}

fn TokenType token_from_identifier(String indentifier)
{
    switch (indentifier) {
        case "asm":
            return ASM;
        case "bitstruct":
            return BITSTRUCT;
        case "break":
            return BREAK;
        case "case":
            return CASE;
        case "catch":
            return CATCH;
        case "const":
            return CONST;
        case "continue":
            return CONTINUE;
        case "alias":
            return ALIAS;
        case "default":
            return DEFAULT;
        case "defer":
            return DEFER;
        case "distinct":
            return DISTINCT;
        case "do":
            return DO;
        case "else":
            return ELSE;
        case "enum":
            return ENUM;
        case "extern":
            return EXTERN;
        case "false":
            return FALSE;
        case "fault":
            return FAULT;
        case "for":
            return FOR;
        case "foreach":
            return FOREACH;
        case "foreach_r":
            return FOREACH_R;
        case "fn":
            return FN;
        case "if":
            return IF;
        case "inline":
            return INLINE;
        case "interface":
            return INTERFACE;
        case "import":
            return IMPORT;
        case "macro":
            return MACRO;
        case "module":
            return MODULE;
        case "nextcase":
            return NEXTCASE;
        case "null":
            return NULL;
        case "return":
            return RETURN;
        case "static":
            return STATIC;
        case "struct":
            return STRUCT;
        case "switch":
            return SWITCH;
        case "true":
            return TRUE;
        case "try":
            return TRY;
        case "typeid":
            return TYPEID;
        case "union":
            return UNION;
        case "while":
            return WHILE;
        case "assert":
            return IDENT;
        case "tlocal":
            return TLOCAL;
            // Types
        case "anyfault":
        case "var":
        case "void":
        case "any":
        case "bool":
        case "float128":
        case "double":
        case "float":
        case "bfloat":
        case "float16":
        case "long":
        case "ulong":
        case "int128":
        case "uint128":
        case "int":
        case "uint":
        case "short":
        case "ushort":
        case "ichar":
        case "char":
        case "isz":
        case "usz":
        case "iptr":
        case "uptr":
            return TYPE_IDENT;
        case "$alignof":
        case "$and":
        case "$append":
        case "$assert":
        case "$assignable":
        case "$concat":
        case "$eval":
        case "$defined":
        case "$embed":
        case "$evaltype":
        case "$error":
        case "$exec":
        case "$extnameof":
        case "$feature":
        case "$is_const":
        case "$include":
        case "$vacount":
        case "$vatype":
        case "$vaconst":
        case "$vaarg":
        case "$varef":
        case "$vaexpr":
        case "$vasplat":
        case "$nameof":
        case "$offsetof":
        case "$or":
        case "$qnameof":
        case "$sizeof":
        case "$typefrom":
        case "$stringify":
        case "$echo":
            return CT_IDENT;
        case "$typeof":
            return CT_TYPEOF;

            // ------
        case "$if":
            return CT_IF;
        case "$else":
            return CT_ELSE;
        case "$endif":
            return CT_ENDIF;
        case "$for":
            return CT_FOR;
        case "$endfor":
            return CT_ENDFOR;
        case "$foreach":
            return CT_FOREACH;
        case "$endforeach":
            return CT_ENDFOREACH;
        case "$switch":
            return CT_SWITCH;
        case "$case":
            return CT_CASE;
        case "$default":
            return CT_DEFAULT;
        case "$endswitch":
            return CT_ENDSWITCH;
        default:
            return INVALID_TOKEN;
    }
}
